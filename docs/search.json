[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am an experienced Data Scientist, AI Engineer and Applied LLM Researcher. I work with language models since 2018 and lead research efforts for training custom language models from scratch on geospatial telecommunications data at O2 Czech Republic. My recent focus has been on retrieval models, rerankers, evaluations, synthetic data generation and agentic search applications.\nOriginally from Germany, I settled in beautiful Prague, Czech Republic. I like learning new things, craft beer, specialty coffee and spending time with my family. I proudly consider myself a repeated fast.ai alumn and follow everything Jeremy Howard now does at Answer.AI with enthusiasm."
  },
  {
    "objectID": "posts/2020-11-22-emnlp/index.html",
    "href": "posts/2020-11-22-emnlp/index.html",
    "title": "EMNLP 2020",
    "section": "",
    "text": "In this blog post I will describe my experience at the 2020 Conference on Empirical Methods in Natural Language Processing, that took place virtually between Nov 16 and Nov 20.\nIt was my first academic ML or NLP conference and I really had a great time. I committed most of my week to the conference and also extended it to the weekend, so that I had two more days to catch up on some papers and talks that I missed during the week. The amount of content that the conference offers in incredible, 752 main conference papers, 25 workshops, 7 tutorials and 5 plenary sessions. Additionally, the conference had plenty of Q&A, gather and demo sessions with paper authors that offer a chance to interact and socialize.\nWhile the information is still fresh, I decided to write down some of my personal highlights from the conference. Of course this only reflects a small subset of the many possibilities. In my role as self-employed NLP data scientist, which necessarily puts me more in the position of a generalist rather than a deeply specialized researcher, I decided to attend as many plenary sessions, tutorials and workshops as possible to get a broad overview of the field and its progress. Even though I’m still relatively new to the field (with a bit more than two years of working in NLP), I could follow along the highly technical content without any problems. I had a somewhat harder time at Q&A and gather sessions, which require detailed knowledge of each discussed topic as well as having read the paper in advance or at least having watched the pre-recorded paper talk. Next time I participate in a virtual conference, I will definitely take a day or two before it starts to better prepare and select the papers I want to learn more about.\n\n\nOf all plenary sessions, which started with a great opening keynote by Claire Cardie from Cornell University on information extraction, my favourite talk was “Friends Don’t Let Friends Deploy Black-Box Models: The Importance of Intelligibility in Machine Learning” by Rich Caruana from Microsoft Research. Even though the talk was actually not about NLP, it was one of the most insightful presentations I experienced at EMNLP.\nIn his captivating talk, Rich Caruana made the claim that in the 2020s, so-called Glass-Box Machine Learning approaches will take over the world of tabular data. In computer vision and NLP on the other hand, where neural networks most likely will keep the upper hand for quite a while, black-box explainability approaches such as LIME and SHAP are the best way to go for interpretability. But back to tabular data. While tree-based models, such as Random Forests already have some nice interpretability features (e.g. partial dependence), Glass-Box ML models go much further. The presented Explainable Boosting Machines, or EBMs - part of the family of Generalized Additive Models (GAMs) - are directly interpretable and even editable. Specifically, the functions that EBMs are fitting to single variables or interactions can be inspected visually in the form of graphs. This offers the ability to really explain model behaviour and get valuable insights about the data, which can be invaluable in sensitive domains such as medical applications. Finally, if unwanted patterns are discovered in a graph, the model can be directly edited to avoid making predictions on wrong patterns. All this comes at a comparable performance to gradient-boosting machines or neural networks for tabular data. This could be a real game changer for the world of tabular data. Here is the repo of InterpretML, which introduces EBMs.\n\n\nMoving on to EMNLP papers it can be said that Transformer-based models have fully taken over. Among main conference papers, a search for “Transformer” or “BERT” yields 39 and 28 results, respectively, while “LSTM”, “RNN” and “CNN” each only return one search result. Because it’s absolutely impossible to make the right choice picking the best papers from such a huge amount of work, I will just briefly mention a couple of papers that I came across and found particularly interesting. Disclaimer: there is definitely a bias towards dialogue systems, since I recently became interested in that field: - Spot The Bot: A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems by Deriu et al. won an Honourable Mention Papers award. They introduce a novel method for evaluating chatbots in which humans have to spot which entity is a bot. - TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue by Wu et al. presents a pre-trained BERT model for task-oriented dialogue that sets new SOTA results for intent classification and response ranking. The pre-trained model has also been open-sourced on the huggingface model hub. - Probing Task-Oriented Dialogue Representation from Language Models by Wu and Xiong evaluates many language models on various dialogue-related tasks, where ConveRT (see tutorials below), TOD-BERT (above) and TOD-GPT2 achieve the best results. - MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems by Lin et al. provides a framework for using pre-trained seq2seq models like T5 and BART for dialogue systems especially for low resource settings. - Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start by Yin et al. proposes textual entailment as a universal method for NLP when annotations are insufficient. - Active Learning for BERT: An Empirical Study by Ein-Dor et al. shows that bringing together transfer learning from pre-trained language models and active learning can help in real-world settings with limited and imbalanced data. - Which BERT? A Survey Organizing Contextualized Encoders by Xia, Wu and Van Durme is a much needed survey of different BERT models that helps you choose the right approach. - The Multilingual Amazon Reviews Corpus by Keung et al. presents a new large-scale corpus of annotated Amazon reviews in multiple languages along with different benchmarks. - Transformers: State-of-the-Art Natural Language Processing by Wolf et al. presents their amazing huggingface libraries and in my opinion deservedly won Best Demo Paper.\n\n\n\n\nIn this section I will briefly describe each of the tutorials and workshops I visited at EMNLP 2020. The bottom line is that I can highly recommend each and every one of these high-quality events. I hope some of the talks will be released to the public in the future. Check the EMNLP and ACL sites for updates.\n\n\nHigh Performance Natural Language Processing\nThis tutorial by Gabriel Ilharco, Cesar Ilharco, Iulia Turc, Tim Dettmers, Felipe Ferreira and Kenton Lee was very well prepared and attracted quite some positive attention among the NLP research community on twitter. The tutorial focused on a variety of techniques to make state-of-the-art transformer models more efficient. While large-scale pre-trained NLP models enabled remarkable progress on many benchmarks, the size and resource requirements of these models pose serious problems regarding practical usability, costs and accessability. Starting out with some fundamentals to understand attention and transformers, the tutorial continued explaining different techniques such as knowledge distillation, quantization and pruning. It then went on to look at the large number of recent papers aimed at making the attention mechanism more efficient, such as Big Bird, Reformer, Performer and Transformer XL. The last part covered the topic of scaling in practice, presenting different opimization techniques and stressing the point that while some optmizations work in theory, in order to really know if they work in practice, you must run them on hardware and see what happens. The slides can be found here.\nThe Amazing World of Neural Language Generation\nThe second tutorial I visited covered all topics related to language generation with neural networks. Yangfeng Ji first walked us through a variety of neural network models for text generation, from basic RNNs through Autoencoders and GANs to Transformers. The next two modules by Antoine Bosselut covered decoding and training NLG models. While the first part of his talk presented different kinds of decoding algorithms, such as Beam Search, top-k and top-p sampling, the second part looked at different training techniques, including reinforcement learning and human-in-the-loop learning such as a recent summarization technique from Open AI. Then, Asli Celikyilmaz covered the important and challenging topic of evaluating neural text generation models. While human evaluation is certainly the gold-standard, it is also very expensive, time-consuming and sometimes inconsistent. Automatic evaluation can be split into untrained metrics, e.g. distance and overlap and trained metrics, e.g. sentence similartiy or learned human feedback. Interesting benchmarking platforms are ParlAI, which is a open-source platform for dialogue research and EvalAI, which supports human-in-the-loop and RL environments. In the final part, Thomas Wolf, CSO at huggingface, went into the challenges when deploying large-scale transformer models. He presented several techniques for optimizing encoders, decoders and decoding algorithms in order to speed up inference. The tutorial slides are available here.\nFact-Checking, Fake News, Propaganda, and Media Bias: Truth Seeking in the Post-Truth Era\nIn this tutorial, Preslav Nakov and Giovanni Da San Martino cover the important topics of disinformation and propaganda. While the title still includes the term “fake news”, Preslav Nakov makes clear early on that we should rather use the term disinformation, which is the overlap of misinformation and malinformation. In other words, disinformation is characterized by falseness and intent to harm. Given the overwhelming amount of news, online discussions and social media posts that are published each day on the internet, using NLP and ML to assist human fact-checkers and/or partially automate the verification process could potentially avoid a lot of harm caused by disinformation campaigns. The tutorial serves both as an introduction to the topic and a presentation of numerous techniques, datasets and challenges. In fact, it covered so much material, that it is impossible to briefly sum up here. But for anyone who is interested in getting started in the field of disinformation I can only recommend to check the slides.\n\n\n\nSCAI 2: Search-Oriented Conversational AI\nThe second SCAI workshop, organized by Julia Kiseleva, Jeff Dalton, Aleksandr Chuklin and Mikhail Burtsev, covered talks and papers related to search-oriented conversational AI. The workshop streamed all pre-recorded talks, followed by live Q&A sessions, to simulate the experience of a live conference. My first highlight was a talk titled “Data-Efficient Natural Language Understanding for Task-Oriented Dialogue”. In his talk, Ivan Vulić from PolyAI addressed the idea of using pre-trained models that encode conversational knowledge to reduce the need for annotated data for building dialogue systems. ConveRT, which also was a findings paper at EMNLP 2020, uses response selection from large conversational datasets as pre-training task. The compact Transformer-based model can be fine-tuned on CPUs and achieves great results on response selection and intent classification. My other highlight was BlenderBot, presented by Jason Weston from Facebook AI Research. The paper presents a recipe for building open-domain chatbots based on large-scale pre-training of Transformer models, fine-tuning and inference optimization. While BlenderBot outperformed Google’s Meena and achieved near-human performance on some tasks, the authors stress that these results should be taken with a grain of salt and the model still makes many typical mistakes such as repetition, inconsistencies or factual incorrectness.\nSustaiNLP: Workshop on Simple and Efficient Natural Language Processing\nI’ve really been looking forward to SustaiNLP and it also was one of the main reasons why I considered attending EMNLP this year. Concerned by the complexity and resource requirements of recent large-scale Transformer models, the workshop’s goal was to promote “simpler and more sustainable NLP research and practices”. My favourite talk at the workshop was “Making Pre-trained Models More Sustainable” by Armand Joulin. The talk starts with the premise that the only thing we know for sure is that we want to simplify pre-trained models - even if we don’t know how exactly in advance. Based on this idea, Transformer models can actually be prepared during pre-training to better deal with simplifications during fine-tuning. A way to do this is to apply structured layer dropout during pre-training, which forces the model to spread out its knowledge more evenly across all layers. When the model size is then reduced at the fine-tuning stage, the model already “knows” how to perform well with a subset of its layers.\nTwo papers that I would recommend from the workshop are SqueezeBERT by Iandola et al., which leverages ideas from computer vision in order to speed up BERT on mobile devices, and FastFormers by Young Jin Kim and Hany Hassan, which combines distillation, pruning, quantization and runtime optimization to achieve a 233x speedup of BERT-base on CPU at less than a 2% drop in accuracy.\n\n\n\n\nI still need some time to process all the new information that I got during the last seven days and I have many papers marked that I still want to read, but all in all I can already now say that I learned a lot. I can only recommend to participate in conferences like this even if you’re (like me) not an experienced NLP researcher. Participating in conferences like EMNLP gives you the unique opportunity to get a detailed impression where the field currently is and look at cutting-edge research from a front row seat. Apart from the already mentioned fact that I want to better prepare in advance for my next conference, my other lesson learned is that I want to set aside more time during the conference to engage socially, make new contacts and exchange ideas.\nI also want to say that the concept of a virtual conference really resonates with me. Of course it lacks the personal experience of physically being in a room together with others, but on the other hand it allows people from all over the world to attend top conferences at a much lower cost. I really hope that hybrid-style conferences with both personal attendance and virtual experience will be offered in the future."
  },
  {
    "objectID": "posts/2020-11-22-emnlp/index.html#main-conference",
    "href": "posts/2020-11-22-emnlp/index.html#main-conference",
    "title": "EMNLP 2020",
    "section": "",
    "text": "Of all plenary sessions, which started with a great opening keynote by Claire Cardie from Cornell University on information extraction, my favourite talk was “Friends Don’t Let Friends Deploy Black-Box Models: The Importance of Intelligibility in Machine Learning” by Rich Caruana from Microsoft Research. Even though the talk was actually not about NLP, it was one of the most insightful presentations I experienced at EMNLP.\nIn his captivating talk, Rich Caruana made the claim that in the 2020s, so-called Glass-Box Machine Learning approaches will take over the world of tabular data. In computer vision and NLP on the other hand, where neural networks most likely will keep the upper hand for quite a while, black-box explainability approaches such as LIME and SHAP are the best way to go for interpretability. But back to tabular data. While tree-based models, such as Random Forests already have some nice interpretability features (e.g. partial dependence), Glass-Box ML models go much further. The presented Explainable Boosting Machines, or EBMs - part of the family of Generalized Additive Models (GAMs) - are directly interpretable and even editable. Specifically, the functions that EBMs are fitting to single variables or interactions can be inspected visually in the form of graphs. This offers the ability to really explain model behaviour and get valuable insights about the data, which can be invaluable in sensitive domains such as medical applications. Finally, if unwanted patterns are discovered in a graph, the model can be directly edited to avoid making predictions on wrong patterns. All this comes at a comparable performance to gradient-boosting machines or neural networks for tabular data. This could be a real game changer for the world of tabular data. Here is the repo of InterpretML, which introduces EBMs.\n\n\nMoving on to EMNLP papers it can be said that Transformer-based models have fully taken over. Among main conference papers, a search for “Transformer” or “BERT” yields 39 and 28 results, respectively, while “LSTM”, “RNN” and “CNN” each only return one search result. Because it’s absolutely impossible to make the right choice picking the best papers from such a huge amount of work, I will just briefly mention a couple of papers that I came across and found particularly interesting. Disclaimer: there is definitely a bias towards dialogue systems, since I recently became interested in that field: - Spot The Bot: A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems by Deriu et al. won an Honourable Mention Papers award. They introduce a novel method for evaluating chatbots in which humans have to spot which entity is a bot. - TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue by Wu et al. presents a pre-trained BERT model for task-oriented dialogue that sets new SOTA results for intent classification and response ranking. The pre-trained model has also been open-sourced on the huggingface model hub. - Probing Task-Oriented Dialogue Representation from Language Models by Wu and Xiong evaluates many language models on various dialogue-related tasks, where ConveRT (see tutorials below), TOD-BERT (above) and TOD-GPT2 achieve the best results. - MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems by Lin et al. provides a framework for using pre-trained seq2seq models like T5 and BART for dialogue systems especially for low resource settings. - Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start by Yin et al. proposes textual entailment as a universal method for NLP when annotations are insufficient. - Active Learning for BERT: An Empirical Study by Ein-Dor et al. shows that bringing together transfer learning from pre-trained language models and active learning can help in real-world settings with limited and imbalanced data. - Which BERT? A Survey Organizing Contextualized Encoders by Xia, Wu and Van Durme is a much needed survey of different BERT models that helps you choose the right approach. - The Multilingual Amazon Reviews Corpus by Keung et al. presents a new large-scale corpus of annotated Amazon reviews in multiple languages along with different benchmarks. - Transformers: State-of-the-Art Natural Language Processing by Wolf et al. presents their amazing huggingface libraries and in my opinion deservedly won Best Demo Paper."
  },
  {
    "objectID": "posts/2020-11-22-emnlp/index.html#tutorials-and-workshops",
    "href": "posts/2020-11-22-emnlp/index.html#tutorials-and-workshops",
    "title": "EMNLP 2020",
    "section": "",
    "text": "In this section I will briefly describe each of the tutorials and workshops I visited at EMNLP 2020. The bottom line is that I can highly recommend each and every one of these high-quality events. I hope some of the talks will be released to the public in the future. Check the EMNLP and ACL sites for updates.\n\n\nHigh Performance Natural Language Processing\nThis tutorial by Gabriel Ilharco, Cesar Ilharco, Iulia Turc, Tim Dettmers, Felipe Ferreira and Kenton Lee was very well prepared and attracted quite some positive attention among the NLP research community on twitter. The tutorial focused on a variety of techniques to make state-of-the-art transformer models more efficient. While large-scale pre-trained NLP models enabled remarkable progress on many benchmarks, the size and resource requirements of these models pose serious problems regarding practical usability, costs and accessability. Starting out with some fundamentals to understand attention and transformers, the tutorial continued explaining different techniques such as knowledge distillation, quantization and pruning. It then went on to look at the large number of recent papers aimed at making the attention mechanism more efficient, such as Big Bird, Reformer, Performer and Transformer XL. The last part covered the topic of scaling in practice, presenting different opimization techniques and stressing the point that while some optmizations work in theory, in order to really know if they work in practice, you must run them on hardware and see what happens. The slides can be found here.\nThe Amazing World of Neural Language Generation\nThe second tutorial I visited covered all topics related to language generation with neural networks. Yangfeng Ji first walked us through a variety of neural network models for text generation, from basic RNNs through Autoencoders and GANs to Transformers. The next two modules by Antoine Bosselut covered decoding and training NLG models. While the first part of his talk presented different kinds of decoding algorithms, such as Beam Search, top-k and top-p sampling, the second part looked at different training techniques, including reinforcement learning and human-in-the-loop learning such as a recent summarization technique from Open AI. Then, Asli Celikyilmaz covered the important and challenging topic of evaluating neural text generation models. While human evaluation is certainly the gold-standard, it is also very expensive, time-consuming and sometimes inconsistent. Automatic evaluation can be split into untrained metrics, e.g. distance and overlap and trained metrics, e.g. sentence similartiy or learned human feedback. Interesting benchmarking platforms are ParlAI, which is a open-source platform for dialogue research and EvalAI, which supports human-in-the-loop and RL environments. In the final part, Thomas Wolf, CSO at huggingface, went into the challenges when deploying large-scale transformer models. He presented several techniques for optimizing encoders, decoders and decoding algorithms in order to speed up inference. The tutorial slides are available here.\nFact-Checking, Fake News, Propaganda, and Media Bias: Truth Seeking in the Post-Truth Era\nIn this tutorial, Preslav Nakov and Giovanni Da San Martino cover the important topics of disinformation and propaganda. While the title still includes the term “fake news”, Preslav Nakov makes clear early on that we should rather use the term disinformation, which is the overlap of misinformation and malinformation. In other words, disinformation is characterized by falseness and intent to harm. Given the overwhelming amount of news, online discussions and social media posts that are published each day on the internet, using NLP and ML to assist human fact-checkers and/or partially automate the verification process could potentially avoid a lot of harm caused by disinformation campaigns. The tutorial serves both as an introduction to the topic and a presentation of numerous techniques, datasets and challenges. In fact, it covered so much material, that it is impossible to briefly sum up here. But for anyone who is interested in getting started in the field of disinformation I can only recommend to check the slides.\n\n\n\nSCAI 2: Search-Oriented Conversational AI\nThe second SCAI workshop, organized by Julia Kiseleva, Jeff Dalton, Aleksandr Chuklin and Mikhail Burtsev, covered talks and papers related to search-oriented conversational AI. The workshop streamed all pre-recorded talks, followed by live Q&A sessions, to simulate the experience of a live conference. My first highlight was a talk titled “Data-Efficient Natural Language Understanding for Task-Oriented Dialogue”. In his talk, Ivan Vulić from PolyAI addressed the idea of using pre-trained models that encode conversational knowledge to reduce the need for annotated data for building dialogue systems. ConveRT, which also was a findings paper at EMNLP 2020, uses response selection from large conversational datasets as pre-training task. The compact Transformer-based model can be fine-tuned on CPUs and achieves great results on response selection and intent classification. My other highlight was BlenderBot, presented by Jason Weston from Facebook AI Research. The paper presents a recipe for building open-domain chatbots based on large-scale pre-training of Transformer models, fine-tuning and inference optimization. While BlenderBot outperformed Google’s Meena and achieved near-human performance on some tasks, the authors stress that these results should be taken with a grain of salt and the model still makes many typical mistakes such as repetition, inconsistencies or factual incorrectness.\nSustaiNLP: Workshop on Simple and Efficient Natural Language Processing\nI’ve really been looking forward to SustaiNLP and it also was one of the main reasons why I considered attending EMNLP this year. Concerned by the complexity and resource requirements of recent large-scale Transformer models, the workshop’s goal was to promote “simpler and more sustainable NLP research and practices”. My favourite talk at the workshop was “Making Pre-trained Models More Sustainable” by Armand Joulin. The talk starts with the premise that the only thing we know for sure is that we want to simplify pre-trained models - even if we don’t know how exactly in advance. Based on this idea, Transformer models can actually be prepared during pre-training to better deal with simplifications during fine-tuning. A way to do this is to apply structured layer dropout during pre-training, which forces the model to spread out its knowledge more evenly across all layers. When the model size is then reduced at the fine-tuning stage, the model already “knows” how to perform well with a subset of its layers.\nTwo papers that I would recommend from the workshop are SqueezeBERT by Iandola et al., which leverages ideas from computer vision in order to speed up BERT on mobile devices, and FastFormers by Young Jin Kim and Hany Hassan, which combines distillation, pruning, quantization and runtime optimization to achieve a 233x speedup of BERT-base on CPU at less than a 2% drop in accuracy."
  },
  {
    "objectID": "posts/2020-11-22-emnlp/index.html#concluding-thoughts",
    "href": "posts/2020-11-22-emnlp/index.html#concluding-thoughts",
    "title": "EMNLP 2020",
    "section": "",
    "text": "I still need some time to process all the new information that I got during the last seven days and I have many papers marked that I still want to read, but all in all I can already now say that I learned a lot. I can only recommend to participate in conferences like this even if you’re (like me) not an experienced NLP researcher. Participating in conferences like EMNLP gives you the unique opportunity to get a detailed impression where the field currently is and look at cutting-edge research from a front row seat. Apart from the already mentioned fact that I want to better prepare in advance for my next conference, my other lesson learned is that I want to set aside more time during the conference to engage socially, make new contacts and exchange ideas.\nI also want to say that the concept of a virtual conference really resonates with me. Of course it lacks the personal experience of physically being in a room together with others, but on the other hand it allows people from all over the world to attend top conferences at a much lower cost. I really hope that hybrid-style conferences with both personal attendance and virtual experience will be offered in the future."
  },
  {
    "objectID": "posts/2020-10-12-nlp_summit/index.html",
    "href": "posts/2020-10-12-nlp_summit/index.html",
    "title": "NLP Summit 2020",
    "section": "",
    "text": "I want to kick off this blog by describing my impressions and take-aways from attending the NLP Summit 2020: Applied Natural Language Processing which took place virtually between October 6th and 9th. John Snow Labs, the main creator behind Spark NLP and organizer of the summit put together a diverse and interesting program for the four conference days.\nOverall I really enjoyed the conference. I especially liked the focus on applied NLP, presenting many specific NLP use cases from industry as well as topics such as product management, model deployment and data annotation. Working myself at the intersection of applied NLP research, data science and engineering, I recognized many common pain points and lessons learned from my own experience during the conference. Please note that this blog post only describes my own experience and doesn’t give a full picture of the conference. Even though I tried to attend as many sessions as possible, I am sure that I still missed some great talks.\n\n\nThe field of applied NLP has made tremendous progress over the last couple of years and continues to grow. Not only are academic benchmarks being beaten at a regular rate, but companies are also increasingly applying these new technologies to their use cases. The field is still very young, with only a minority of companies having NLP models in production for more than five years. However, companies are investing. According to the 2020 NLP Survey Report by Ben Lorica and Paco Nathan, “53% of respondents who are Technical Leaders stated their NLP budget was at least 10% higher compared to 2019”.\nWhen it comes to specific use cases, document classification, named entity recognition and sentiment analysis are still the most popular NLP tasks. Given the sheer amount of text documents that companies are processing on a daily basis and the recent rate of improvement of NLP techniques, I believe that we will see an enourmous demand for NLP tools, platforms, engineers and researchers in the coming years. Clément Delangue, CEO at huggingface, even went so far as giving a keynote titled “NLP is going to be the most transformational tech of the decade!”, basing this claim on the fact that most of our day, both at work and in private, is spent using natural language, which finally can be accurately processed by machines thanks to recent improvements.\nJust to mention a few of the many exciting use cases that were presented at the conference: - automated de-identification of medial records to enable the processing of sensitive documents such as in healthcare - above-human-level-performance document classification and abstractive (!) summarization of documents in the legal domain - using ML-powered intent recognition, curated templates and knowledge graphs to build effective chatbots\nWhich tools are used in industry?\nEven though from following the Deep Learning community on twitter one might have the impression that everyone uses the huggingface transformers library (disclaimer: I’m a big fan myself) the reality paints a somewhat different picture. According to the above-mentioned survey, the three most-used NLP libraries are Spark NLP, spaCy and Allen NLP, with huggingface landing on rank 7 (which is still very impressive for such a young library). Which tool is the right fit for a given project strongly depends on the specific requirements. Spark NLP and spaCy provide easy-to-use, reliable and efficient libraries for a variety of practical NLP use cases. Allen NLP might be a better suited for researchers. For state-of-the-art pre-trained transformer models one very likely would choose huggingface transformers. For topic modelling gensim is still very popular. For part-of-speech-tagging or dependency parsing Stanford CoreNLP might be the best choice. A newcomer is the Berlin-based starup Rasa that provides an open source framework and a tool called Rasa X for developing contextual AI assistants.\nEven though a majority of companies use at least one of the leading NLP cloud services, there are still many challenges to be considered. Among them are price, missing customizability and features, low accuracy, unwillingness to share data and missing support of certain languages.\nPlease download and have a look at the 2020 NLP Survey Report for more details.\n\n\n\nData annotation or labelling is often considered an uncool, unqualified and strenuous task that has to be completed before finally being able to train a fancy model. However, in reality it is one of the most crucial tasks of completing a successful machine learning project that deserves more attention. Without clear guidelines and best practices, data annotation can become very expensive and even lead to the failure of machine learning projects due to insufficient data quality. It was great to see the topic being discussed repeatedly during several session at the NLP Summit 2020. Rebecca Leung and Marianne Mak from John Snow Labs even gave an entire talk about “Lessons learned annotating training data for healthcare NLP projects”. Here are some of their insights as well as ideas from other talks and my own experience.\nData annotation should be done by experts! While it may be relatively easy to draw a bounding box around a street sign, most NLP annotation tasks require serious skills. If you want to have a good dataset, annotation needs to be done by qualified domain-experts. For sensitive areas such as healthcare or legal, this might even require having a PhD in the respective field. Moreover, annotation requires incredible focus, speed and endurance. Great annotators should be appreciated and well compensated.\nGo for quality, not quantity! Even if you have experts annotating your data, they can still make some mistakes. Some label types (e.g. sentiment) are very subjective and depend on the opinion of a given annotator. Training a machine learning model on smaller, high-quality datasets can lead to better results than training on larger, but noisy and inconsistent data. If the annotation task is subjective or ambiguous, it may help to let two (or more) annotators label the same examples and go by consensus or majority voting.\nDefine and review clear annotation guidelines! It is essential to clearly define guidelines for each annotation project. Formulating guidelines helps current annotators structure their thoughts and future annotators during onboarding. However, these guidelines are not set in stone but can evolve over time. It is important to hold regular (e.g. weekly) review sessions with team members, data scientists and clients to make sure everybody is and stays aligned.\nUse annotation tools!!! If there is only one lesson learned about data annotation it is to use specialized tools. It is common best practice to use software tools to significantly speed up the annotation process by providing a clear interface, keyboard shortcuts and workflow support. Moreover, putting the model in the loop and using active learning techniques can make annotators even more efficient. Companies either build their own tools or use commerical ones, such as Prodigy from the makers of spaCy.\n\n\n\nIn this last section I want to briefly summarize my three personal highlights from the conference.\nJoel Grus, Principal Engineer at Capital Group, gave an insightful and entertaining keynote titled “Proof-of-Concept delight”. Drawing on his own experience, Joel demonstrated how to build a PoC in about four hours using modern NLP tools. Here are the main steps he follows: 1. Identify the business problem: find out what task needs to be solved 2. Find the ML problem: formulate the task as a machine learning problem 3. Find a dataset: either you already have data or you need to find a suitable dataset 4. Scope down relentlessly: simplify the problem until it is the simplest useful version 5. Create a data model: Joel prefers a typed representation using NamedTuple over pandas for NLP datasets, since those often do not come in a tabular structure but rather have a one-to-many relationship, e.g. one line of text has multiple labels or named entities 6. Explore and clean your data: don’t spend ages here but do the necessary cleanup 7. Get labels: either from the dataset or create labels yourself using Prodigy or Snorkel 8. Choose a really simple model and adapt it just enough to work on your problem, e.g. spaCy’s off-the-shelf text classification model 9. Train the model and evaluate on a hold out set using the right metrics 10. Build a very simple demo web app with a text field and functionality to return predictions. Streamlit makes is extremely easy 11. Take a risk and give the demo to customers. If they like it, build a production version. If not go back and improve the PoC.\nChristine Gerpheide, CTO at Bespoke, presented two case studies about taking NLP from research to production. After giving a crash course on building chatbots, Christine walked us through the steps Bespoke takes before putting NLP models in production: 1. Identify opportunities for applying machine learning. It’s ok to use simple methods at this stage, e.g. pattern matching using regular expression 2. Do research, then build a prototype. Benchmark different approaches, e.g. baseline vs. custom development vs. cloud service 3. Have a go/no go meeting. Decide if there is potential and which approach to follow 4. Build a Minimum Viable Product and test it on a sub-set of users to get real feedback 5. Improve based on feedback, clean up the code and roll-out to all users\nThe other use case presented was predictive typing. Christine also spent some time talking about the advantages of using in-house machine learning development rather than commercial services. Among them were the possibility to customize models, inspect how models are behaving and better control of model outputs.\nMoshe Wasserblat, NLP & DL Research Manager at the Intel AI Lab, gave a talk on the efficient use of deep learning in production. Given the size and computational cost of recent transformer-based language models (think of T5, Turing-NLG and especially GPT-3) it is important to also consider the practicality of implementing these models in production. There are several attempts to make transformer models smaller, such as quantization, pruning, early prediction, weight sharing or distillation. DistilBERT from huggingface is 40% smaller and 60% faster than BERT, while maintaining 97% of its language understanding capabilities. Other even smaller yet powerful transformer models are MobileBERT, TinyBERT and aLBERT, which are easier to fine-tune and deploy. For extreme compression and inference speed up, it is even possible to distil BERT into simpler models like LSTMs, CNNs or CBOW models. Personally I am very excited about the topic of making state-of-the-art NLP models more efficient for practical purposes. There is more to come at the SustaiNLP workshop at EMNLP 2020.\nThanks for reading! Please let me know your feedback!"
  },
  {
    "objectID": "posts/2020-10-12-nlp_summit/index.html#the-state-of-applied-nlp-in-2020",
    "href": "posts/2020-10-12-nlp_summit/index.html#the-state-of-applied-nlp-in-2020",
    "title": "NLP Summit 2020",
    "section": "",
    "text": "The field of applied NLP has made tremendous progress over the last couple of years and continues to grow. Not only are academic benchmarks being beaten at a regular rate, but companies are also increasingly applying these new technologies to their use cases. The field is still very young, with only a minority of companies having NLP models in production for more than five years. However, companies are investing. According to the 2020 NLP Survey Report by Ben Lorica and Paco Nathan, “53% of respondents who are Technical Leaders stated their NLP budget was at least 10% higher compared to 2019”.\nWhen it comes to specific use cases, document classification, named entity recognition and sentiment analysis are still the most popular NLP tasks. Given the sheer amount of text documents that companies are processing on a daily basis and the recent rate of improvement of NLP techniques, I believe that we will see an enourmous demand for NLP tools, platforms, engineers and researchers in the coming years. Clément Delangue, CEO at huggingface, even went so far as giving a keynote titled “NLP is going to be the most transformational tech of the decade!”, basing this claim on the fact that most of our day, both at work and in private, is spent using natural language, which finally can be accurately processed by machines thanks to recent improvements.\nJust to mention a few of the many exciting use cases that were presented at the conference: - automated de-identification of medial records to enable the processing of sensitive documents such as in healthcare - above-human-level-performance document classification and abstractive (!) summarization of documents in the legal domain - using ML-powered intent recognition, curated templates and knowledge graphs to build effective chatbots\nWhich tools are used in industry?\nEven though from following the Deep Learning community on twitter one might have the impression that everyone uses the huggingface transformers library (disclaimer: I’m a big fan myself) the reality paints a somewhat different picture. According to the above-mentioned survey, the three most-used NLP libraries are Spark NLP, spaCy and Allen NLP, with huggingface landing on rank 7 (which is still very impressive for such a young library). Which tool is the right fit for a given project strongly depends on the specific requirements. Spark NLP and spaCy provide easy-to-use, reliable and efficient libraries for a variety of practical NLP use cases. Allen NLP might be a better suited for researchers. For state-of-the-art pre-trained transformer models one very likely would choose huggingface transformers. For topic modelling gensim is still very popular. For part-of-speech-tagging or dependency parsing Stanford CoreNLP might be the best choice. A newcomer is the Berlin-based starup Rasa that provides an open source framework and a tool called Rasa X for developing contextual AI assistants.\nEven though a majority of companies use at least one of the leading NLP cloud services, there are still many challenges to be considered. Among them are price, missing customizability and features, low accuracy, unwillingness to share data and missing support of certain languages.\nPlease download and have a look at the 2020 NLP Survey Report for more details."
  },
  {
    "objectID": "posts/2020-10-12-nlp_summit/index.html#on-the-importance-of-data-annotation",
    "href": "posts/2020-10-12-nlp_summit/index.html#on-the-importance-of-data-annotation",
    "title": "NLP Summit 2020",
    "section": "",
    "text": "Data annotation or labelling is often considered an uncool, unqualified and strenuous task that has to be completed before finally being able to train a fancy model. However, in reality it is one of the most crucial tasks of completing a successful machine learning project that deserves more attention. Without clear guidelines and best practices, data annotation can become very expensive and even lead to the failure of machine learning projects due to insufficient data quality. It was great to see the topic being discussed repeatedly during several session at the NLP Summit 2020. Rebecca Leung and Marianne Mak from John Snow Labs even gave an entire talk about “Lessons learned annotating training data for healthcare NLP projects”. Here are some of their insights as well as ideas from other talks and my own experience.\nData annotation should be done by experts! While it may be relatively easy to draw a bounding box around a street sign, most NLP annotation tasks require serious skills. If you want to have a good dataset, annotation needs to be done by qualified domain-experts. For sensitive areas such as healthcare or legal, this might even require having a PhD in the respective field. Moreover, annotation requires incredible focus, speed and endurance. Great annotators should be appreciated and well compensated.\nGo for quality, not quantity! Even if you have experts annotating your data, they can still make some mistakes. Some label types (e.g. sentiment) are very subjective and depend on the opinion of a given annotator. Training a machine learning model on smaller, high-quality datasets can lead to better results than training on larger, but noisy and inconsistent data. If the annotation task is subjective or ambiguous, it may help to let two (or more) annotators label the same examples and go by consensus or majority voting.\nDefine and review clear annotation guidelines! It is essential to clearly define guidelines for each annotation project. Formulating guidelines helps current annotators structure their thoughts and future annotators during onboarding. However, these guidelines are not set in stone but can evolve over time. It is important to hold regular (e.g. weekly) review sessions with team members, data scientists and clients to make sure everybody is and stays aligned.\nUse annotation tools!!! If there is only one lesson learned about data annotation it is to use specialized tools. It is common best practice to use software tools to significantly speed up the annotation process by providing a clear interface, keyboard shortcuts and workflow support. Moreover, putting the model in the loop and using active learning techniques can make annotators even more efficient. Companies either build their own tools or use commerical ones, such as Prodigy from the makers of spaCy."
  },
  {
    "objectID": "posts/2020-10-12-nlp_summit/index.html#my-personal-highlights",
    "href": "posts/2020-10-12-nlp_summit/index.html#my-personal-highlights",
    "title": "NLP Summit 2020",
    "section": "",
    "text": "In this last section I want to briefly summarize my three personal highlights from the conference.\nJoel Grus, Principal Engineer at Capital Group, gave an insightful and entertaining keynote titled “Proof-of-Concept delight”. Drawing on his own experience, Joel demonstrated how to build a PoC in about four hours using modern NLP tools. Here are the main steps he follows: 1. Identify the business problem: find out what task needs to be solved 2. Find the ML problem: formulate the task as a machine learning problem 3. Find a dataset: either you already have data or you need to find a suitable dataset 4. Scope down relentlessly: simplify the problem until it is the simplest useful version 5. Create a data model: Joel prefers a typed representation using NamedTuple over pandas for NLP datasets, since those often do not come in a tabular structure but rather have a one-to-many relationship, e.g. one line of text has multiple labels or named entities 6. Explore and clean your data: don’t spend ages here but do the necessary cleanup 7. Get labels: either from the dataset or create labels yourself using Prodigy or Snorkel 8. Choose a really simple model and adapt it just enough to work on your problem, e.g. spaCy’s off-the-shelf text classification model 9. Train the model and evaluate on a hold out set using the right metrics 10. Build a very simple demo web app with a text field and functionality to return predictions. Streamlit makes is extremely easy 11. Take a risk and give the demo to customers. If they like it, build a production version. If not go back and improve the PoC.\nChristine Gerpheide, CTO at Bespoke, presented two case studies about taking NLP from research to production. After giving a crash course on building chatbots, Christine walked us through the steps Bespoke takes before putting NLP models in production: 1. Identify opportunities for applying machine learning. It’s ok to use simple methods at this stage, e.g. pattern matching using regular expression 2. Do research, then build a prototype. Benchmark different approaches, e.g. baseline vs. custom development vs. cloud service 3. Have a go/no go meeting. Decide if there is potential and which approach to follow 4. Build a Minimum Viable Product and test it on a sub-set of users to get real feedback 5. Improve based on feedback, clean up the code and roll-out to all users\nThe other use case presented was predictive typing. Christine also spent some time talking about the advantages of using in-house machine learning development rather than commercial services. Among them were the possibility to customize models, inspect how models are behaving and better control of model outputs.\nMoshe Wasserblat, NLP & DL Research Manager at the Intel AI Lab, gave a talk on the efficient use of deep learning in production. Given the size and computational cost of recent transformer-based language models (think of T5, Turing-NLG and especially GPT-3) it is important to also consider the practicality of implementing these models in production. There are several attempts to make transformer models smaller, such as quantization, pruning, early prediction, weight sharing or distillation. DistilBERT from huggingface is 40% smaller and 60% faster than BERT, while maintaining 97% of its language understanding capabilities. Other even smaller yet powerful transformer models are MobileBERT, TinyBERT and aLBERT, which are easier to fine-tune and deploy. For extreme compression and inference speed up, it is even possible to distil BERT into simpler models like LSTMs, CNNs or CBOW models. Personally I am very excited about the topic of making state-of-the-art NLP models more efficient for practical purposes. There is more to come at the SustaiNLP workshop at EMNLP 2020.\nThanks for reading! Please let me know your feedback!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my blog! Check this short post about what you can expect to read here. Hint: it’s all about retrieval.\nI will finally kick off this blog and plan to post about these topics in the following weeks and months:\n\nhow are retrieval models trained?\nhow do late interaction models work?\nwhat are the latest trends in retrieval?\n\nThese posts will include paper reviews, tutorials and research.\nLooking back at the two old posts below from 2020, when I was still relatively new in the field, I can say that a lot as changed since then. We have a lot to cover, so let’s not waste any time and get right to it.\nThanks for reading. I hope you enjoy it!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Articles",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nAug 29, 2025\n\n\nStefan Josef\n\n\n\n\n\n\n\n\n\n\n\n\nEMNLP 2020\n\n\n\nnlp\n\nconferences\n\n\n\n\n\n\n\n\n\nNov 22, 2020\n\n\nStefan Josef\n\n\n\n\n\n\n\n\n\n\n\n\nNLP Summit 2020\n\n\n\nnlp\n\nconferences\n\n\n\n\n\n\n\n\n\nOct 12, 2020\n\n\nStefan Josef\n\n\n\n\n\nNo matching items"
  }
]